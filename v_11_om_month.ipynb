{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This version uses the same logic as in v_10 but uses one week as its timeframe\n",
    "\n",
    "#### Have also updated the FCR and aFRR functions to make them faster and make FCR handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "import pandas as pd\n",
    "from code_map import final_markets, new_meters, utils, analysis, timeframes, data_handling\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = timeframes.one_week\n",
    "L, M, F, H, freq_data, power_meter_dict, consumption_data = utils.get_all_sets(timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_u, L_d, Fu_h_l, Fd_h_l, R_h_l, P_h_m, Vp_h_m, Vm_m, R_m = utils.get_parameters(L,M,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "markets_name_dict = {market.name: market for market in M}\n",
    "market_names = list(markets_name_dict.keys())\n",
    "price_list = [market.price_data for market in M]\n",
    "volume_list = [market.volume_data for market in M]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of markets : {len(M)}\")\n",
    "print(f\"Amount of meters : {len(L)}\")\n",
    "print(f\"Amount of meters with direction up or both : {len(L_u)}\")\n",
    "print(f\"Amount of meters with direction down or both : {len(L_d)}\")\n",
    "print(f\"Amount of hours : {len(H)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_up_flex = np.sum(Fu_h_l) # total available flex volume up\n",
    "total_down_flex = np.sum(Fd_h_l) # total available flex volume down\n",
    "total_response_time = np.sum(R_h_l) # total response time\n",
    "#total_flex = total_up_flex + total_down_flex\n",
    "average_response_time = total_response_time/ (len(H)*len(L))\n",
    "hourly_flex_up = total_up_flex/len(H)\n",
    "hourly_flex_down = total_down_flex/len(H)\n",
    "\n",
    "print(f\"Total up flex volume: {total_up_flex} MW\")\n",
    "print(f\"Total down flex volume: {total_down_flex} MW\")\n",
    "print(f\"Average flex volume pr hour up: {hourly_flex_up} MWh\")\n",
    "print(f\"Average flex volume pr hour down: {hourly_flex_down} MWh\")\n",
    "print(f\"Average response time: {average_response_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_directions = [utils.get_dominant_direction(freq_data, hour) for hour in H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ir_hlm, Ia_hlm, Va_hm = utils.get_income_dictionaries(H = H, M = M, L = L, freq_data= freq_data, Fu_h_l = Fu_h_l, Fd_h_l = Fd_h_l, P_h_m = P_h_m, Vp_h_m = Vp_h_m, F = F, markets_dict= markets_name_dict, timeframe = timeframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_list = utils.get_compatibility_dict(L, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_batched_versions(L : [new_meters.PowerMeter], M : [final_markets.ReserveMarket], H : [pd.Timestamp], F : dict, freq_data :pd.DataFrame, P_h_m : np.array, Vp_h_m : np.array, R_h_l : np.array, Fu_h_l : np.array, Fd_h_l : np.array):\n",
    "    \"\"\" Function to only fetch the collections that are batched in the batched optimization model. Can use this to check if there is something wrong with some of the batches.\n",
    "\n",
    "    Args:\n",
    "        L (list(new_meters.PowerMeter]): set of all meters\n",
    "        M (list(final_markets.ReserveMarket]): set of all markets\n",
    "        H (list(pd.Timestamp]): set of all hours\n",
    "        F (dict): Dictionary to find the activation percentages for each market and hour\n",
    "        freq_data (pd.DataFrame): dataframe with the frequency data\n",
    "        P_h_m (np.array): The price for each hour and market\n",
    "        Vp_h_m (np.array): The volume for each hour and market\n",
    "        R_h_l (np.array): Response time for each load each hour\n",
    "        Fu_h_l (np.array): Up flex volume for each load that are compatible with up markets for each hour \n",
    "        Fd_h_l (np.array): Down flex volume for each load that are compatible with down markets for each hour\n",
    "       \n",
    "\n",
    "    Returns:\n",
    "        batched_results (dict): Dictionary with the batched results\n",
    "    \"\"\"\n",
    "    batch_size = 24  # For example, batching by 24 hours\n",
    "    num_batches = math.ceil(len(H) / batch_size)\n",
    "    aggregated_results = {\n",
    "        'batched H': [],\n",
    "        'batched R_h_l': [],\n",
    "        'batched Fu_h_l': [],\n",
    "        'batched Fd_h_l': [],\n",
    "        'batched Vp_h_m': [],\n",
    "        'batched P_h_m': [],\n",
    "        'batched timeframes': [],\n",
    "        'batched Ir_h_l_m': [],\n",
    "        'batched Ia_h_l_m': [],\n",
    "        'batched Va_hm': []\n",
    "    }\n",
    "    market_name_dict = {m.name : m for m in M}\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        # Determine the subset of hours for this batch\n",
    "        start_index = b * batch_size\n",
    "        end_index = min((b + 1) * batch_size, len(H))\n",
    "        batch_H = H[start_index:end_index]\n",
    "\n",
    "        # Slice numpy arrays for the current batch\n",
    "        batch_R_h_l = R_h_l[start_index:end_index, :]\n",
    "        batch_Fu_h_l = Fu_h_l[start_index:end_index, :]\n",
    "        batch_Fd_h_l = Fd_h_l[start_index:end_index, :]\n",
    "        batch_Vp_h_m = Vp_h_m[start_index:end_index, :]\n",
    "        batch_P_h_m = P_h_m[start_index:end_index, :]\n",
    "        tf = timeframes.TimeFrame(year = 2023, start_month = 6, end_month = 6, start_day = batch_H[0].day, end_day = batch_H[0].day, start_hour = 0, end_hour = 23)\n",
    "\n",
    "        # the income\n",
    "        batch_Ir_hlm, batch_Ia_hlm, batch_Va_hm = utils.get_income_dictionaries(H = batch_H, M = M, L =L, freq_data = freq_data, Fu_h_l = batch_Fu_h_l, Fd_h_l = batch_Fd_h_l, P_h_m = batch_P_h_m, Vp_h_m = batch_Vp_h_m, F =F, markets_dict = market_name_dict, timeframe = tf)\n",
    "       \n",
    "\n",
    "        # Run the optimization model for this batch\n",
    "        #_, x, y, w, d = run_optimization_model(L= L, M= M, H = batch_H,F= F, Ir_hlm= batch_Ir_hlm, Ia_hlm= batch_Ia_hlm, Va_hm= batch_Va_hm, Vp_h_m= batch_Vp_h_m, Vm_m=Vm_m, R_m=R_m, R_h_l=batch_R_h_l, Fu_h_l=batch_Fu_h_l, Fd_h_l=batch_Fd_h_l, compatible_list=compatible_list, log_filename=log_filename, model_name=f\"{model_name}_batch_{b}\")\n",
    "        # Store results\n",
    "        #aggregated_results['models'].append(model)\n",
    "        aggregated_results['batched H'].append(batch_H)\n",
    "        aggregated_results['batched R_h_l'].append(batch_R_h_l)\n",
    "        aggregated_results['batched Fu_h_l'].append(batch_Fu_h_l)\n",
    "        aggregated_results['batched Fd_h_l'].append(batch_Fd_h_l)\n",
    "        aggregated_results['batched Vp_h_m'].append(batch_Vp_h_m)\n",
    "        aggregated_results['batched P_h_m'].append(batch_P_h_m)\n",
    "        aggregated_results['batched timeframes'].append(tf)\n",
    "        aggregated_results['batched Ir_h_l_m'].append(batch_Ir_hlm)\n",
    "        aggregated_results['batched Ia_h_l_m'].append(batch_Ia_hlm)\n",
    "        aggregated_results['batched Va_hm'].append(batch_Va_hm)\n",
    "        \n",
    "\n",
    "\n",
    "    # Process aggregated_results as needed\n",
    "    return aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vals = get_batched_versions(L= L, M= M, H = H, F = F,freq_data=freq_data, P_h_m=P_h_m , Vp_h_m =Vp_h_m, R_h_l = R_h_l, Fu_h_l = Fu_h_l, Fd_h_l = Fd_h_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for day in range(len(batch_vals[\"batched H\"])):\n",
    "\n",
    "\"\"\"\n",
    "'batched H': [],\n",
    "'batched R_h_l': [],\n",
    "'batched Fu_h_l': [],\n",
    "'batched Fd_h_l': [],\n",
    "'batched Vp_h_m': [],\n",
    "'batched P_h_m': [],\n",
    "'batched timeframes': [],\n",
    "'batched Ir_h_l_m': [],\n",
    "'batched Ia_h_l_m': [],\n",
    "'batched Va_hm': []\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for h in range(len(batch_vals[\"batched H\"][6])):\n",
    "    for l in range(len(L)):\n",
    "        \"\"\"if np.isnan(batch_vals[\"batched R_h_l\"][6][h,l]):\n",
    "                print(f\" R_h_l is nan for Hour: {h}, Meter: {l}\")\n",
    "        if np.isnan(batch_vals[\"batched Fu_h_l\"][6][h,l]):\n",
    "            print(f\" Fu_h_l is nan for Hour: {h}, Meter: {l}\")\n",
    "        if np.isnan(batch_vals[\"batched Fd_h_l\"][6][h,l]):\n",
    "            print(f\"Fd_h_l is nan for Hour: {h}, Meter: {l}\")\"\"\"\n",
    "        for m in range(12,22):\n",
    "            \"\"\"if np.isnan(batch_vals[\"batched Vp_h_m\"][6][h,m]):\n",
    "                print(f\" Vp_h_m is nan for Hour: {h}, Meter: {l}, Market: {m}\")\n",
    "            if np.isnan(batch_vals[\"batched P_h_m\"][6][h,m]):\n",
    "                print(f\" P_h_m is nan for Hour: {h}, Meter: {l}, Market: {m}\")\n",
    "            if np.isnan(batch_vals[\"batched Va_hm\"][6][h,m]):\n",
    "                print(f\" Va_hm is nan for Hour: {h}, Meter: {l}, Market: {m}\")\"\"\"\n",
    "                \n",
    "        \n",
    "            if np.isnan(batch_vals[\"batched Ia_h_l_m\"][6][h,l,m]):\n",
    "                print(f\" Ia_hlm is nan for Hour: {h}, Meter: {l}, Market: {m}\")\n",
    "            if np.isinf(batch_vals[\"batched Ia_h_l_m\"][6][h,l,m]):\n",
    "                print(f\" Ia_hlm is inf for Hour: {h}, Meter: {l}, Market: {m}\")\n",
    "            if np.isnan(batch_vals[\"batched Ir_h_l_m\"][6][h,l,m]):\n",
    "                print(f\" Ir_hlm is nan for Hour: {h}, Meter: {l}, Market: {m}\")\n",
    "            if np.isinf(batch_vals[\"batched Ir_h_l_m\"][6][h,l,m]):\n",
    "                print(f\" Ir_hlm is inf for Hour: {h}, Meter: {l}, Market: {m}\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is only problems with the Ia_hlm and none of the other collections. Ia_hlm has nan values for markets 12-21 which is the fcr_n markets. I have to figure out why this problem is occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results  = utils.run_batched_optimization_model(L= L , M = M, H = H, F = F,freq_data=freq_data, P_h_m=P_h_m , Vp_h_m =Vp_h_m, Vm_m = Vm_m, R_m = R_m, R_h_l = R_h_l, Fu_h_l = Fu_h_l, Fd_h_l = Fd_h_l, compatible_list = compatible_list, log_filename=\"batched_week_v4.log\", model_name= \"weekly_model_batched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissaggregate_results(aggregated_results : dict):\n",
    "    daily_vals = []\n",
    "    x_vals = aggregated_results['x_values']\n",
    "    y_vals = aggregated_results['y_values']\n",
    "    w_vals = aggregated_results['w_values']\n",
    "    for day_nr in range(len(x_vals)):\n",
    "        daily_vals.append({\"x_values\": x_vals[day_nr], \"y_values\": y_vals[day_nr], \"w_values\": w_vals[day_nr]})\n",
    "    return daily_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_vals_list = dissaggregate_results(aggregated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_solution_validity(x, y, w, Va_hm, L, M, H, dominant_directions, F):\n",
    "    \"\"\" function to test the validity of the solution provided by a solver\n",
    "\n",
    "    Args:\n",
    "        x (dict): dictionary of the binary variable which tells if an asset is connected to a market\n",
    "        y (dict): dictionary of the binary variable which tells if a market has any bids\n",
    "        w (dict): dictionary of the binary variable which tells if a market is activated\n",
    "        L (list(PowerMeter)): list of powermeter objects with the data for each meter within the timeframe\n",
    "        M (list(ReserveMarket)): list of reservemarket objects with the data for each market within the timeframe\n",
    "        H (list(pd.TimeStamp)): list of hourly timestamps within the timeframe\n",
    "        dominant_directions (list(str)): list of the dominant direction for each hour\n",
    "        F (pd.DataFrame): dictionary for frequency data\n",
    "    Returns:\n",
    "        str : a string that tells if the solution is valid. If not valid, the function will raise an error\n",
    "    \"\"\"\n",
    "    for h, hour in enumerate(H):\n",
    "        for l, load in enumerate(L):\n",
    "            # Each asset can only be connected to one market at a time\n",
    "            assert round(sum(x[h, l, m].X for m in range(len(M))), 5) <= 1, f\"Asset {l} connected to multiple markets at hour {h}\"\n",
    "            for m, market in enumerate(M):\n",
    "                x_val= round(x[h, l, m].X, 5)\n",
    "                # Directionality constraints\n",
    "                if load.direction == \"up\" and market.direction == \"down\":\n",
    "                    assert x_val== 0, f\"Up-direction asset {l} connected to down-direction market {m} at hour {h}\"\n",
    "                elif load.direction == \"down\" and market.direction == \"up\":\n",
    "                    assert x_val == 0, f\"Down-direction asset {l} connected to up-direction market {m} at hour {h}\"\n",
    "                #elif market.direction == \"both\" and load.direction != \"both\":\n",
    "                    #assert x[h, l, m].X == 0, f\"Asset {l} with specific direction connected to both-direction market {m} at hour {h}\"\n",
    "                elif market.area != load.area:\n",
    "                    assert x_val == 0, f\"Asset {l} in area {load.area} connected to market {m} in area {market.area} at hour {h}\"\n",
    "                \n",
    "                # Response time constraints\n",
    "                assert x_val * load.response_time <= market.response_time * round(y[h, m].X, 5), f\"Asset {l} connected to market {m} at hour {h} violates response time constraint\"\n",
    "                \n",
    "        for m, market in enumerate(M):\n",
    "            # Connect the binary variables by using big M\n",
    "            assert round(sum(x[h, l, m].X for l in range(len(L))), 5) <= len(L) * round(y[h, m].X, 5), f\"More than allowed assets connected to market {m} at hour {h} to market {m}\"\n",
    "\n",
    "            #total_flex_volume = sum(x[h, l, m].X * load.flex_volume[\"value\"].loc[load.flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L))\n",
    "\n",
    "            # Min volume constraint\n",
    "            if market.direction == \"up\":\n",
    "                total_flex_volume = sum(x[h, l, m].X * load.up_flex_volume[\"value\"].loc[load.up_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"down\")\n",
    "            elif market.direction == \"down\":\n",
    "                total_flex_volume = sum(x[h, l, m].X * load.down_flex_volume[\"value\"].loc[load.down_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"up\")\n",
    "            else: # direction = \"both\"\n",
    "                if dominant_directions[h] == \"up\":\n",
    "                    total_flex_volume = sum(x[h, l, m].X * load.up_flex_volume[\"value\"].loc[load.up_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"down\")\n",
    "                else:\n",
    "                    total_flex_volume = sum(x[h, l, m].X * load.down_flex_volume[\"value\"].loc[load.down_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"up\")\n",
    "            \n",
    "            assert round(total_flex_volume, 5) >= market.min_volume * y[h, m].X, f\"Minimum volume constraint violated at hour {h} for market {m}\"\n",
    "            \n",
    "            # Max volume constraint for both capacity and activation\n",
    "            if market.direction == \"up\":\n",
    "                total_max_volume = sum(x[h, l, m].X * load.up_flex_volume[\"value\"].loc[load.up_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"down\")\n",
    "\n",
    "            elif market.direction == \"down\":\n",
    "                total_max_volume = sum(x[h, l, m].X * load.down_flex_volume[\"value\"].loc[load.down_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"up\")\n",
    "\n",
    "            else:\n",
    "                \"\"\"if dominant_directions[h] == \"up\":\n",
    "                    total_max_volume = sum(x[h, l, m].X * load.up_flex_volume[\"value\"].loc[load.up_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"down\")\n",
    "                else:\n",
    "                    total_max_volume = sum(x[h, l, m].X * load.down_flex_volume[\"value\"].loc[load.down_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"up\")\"\"\"\n",
    "                total_up_max_volume = sum(x[h, l, m].X * load.up_flex_volume[\"value\"].loc[load.up_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"down\")\n",
    "                total_down_max_volume = sum(x[h, l, m].X * load.down_flex_volume[\"value\"].loc[load.down_flex_volume[\"Time(Local)\"] == hour].values[0] for l, load in enumerate(L) if load.direction != \"up\")\n",
    "                up_frac, down_frac = F[h,m]\n",
    "                total_max_volume = (total_up_max_volume * up_frac + total_down_max_volume * down_frac)\n",
    "            \n",
    "             # Assert the constraints\n",
    "            activation_constraint = round(total_max_volume, 5)  * round(w[h,m].X, 5) <= Va_hm[h,m]\n",
    "            assert activation_constraint, f\"Activation constraint violated for hour {h}, market {m}\"\n",
    "            market_max_volume = market.volume_data.loc[market.volume_data[\"Time(Local)\"] == hour].values[0][1]\n",
    "            assert total_max_volume <= market_max_volume * round(y[h,m].X, 5), f\"Maximum volume constraint violated at hour {h} for market {m}\"\n",
    "    return \"Solution is valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_batches = [H[:24], H[24:]]\n",
    "for index, daily_val in enumerate(daily_vals_list):\n",
    "    utils.test_solution_validity(daily_val[\"x_values\"], daily_val[\"y_values\"], daily_val[\"w_values\"], Va_hm, L, M, H_batches[index], dominant_directions= dominant_directions, F = F)\n",
    "    print(f\" Solution for day {daily_vals_list.index(daily_val)} is valid\")\n",
    "    # must split the H list into days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### possible reasons for failure of the second day batched version:\n",
    "# the indexes becomes wrong when the data is batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afrr_activation_up = utils.get_afrr_activation_data(tf = timeframe, afrr_directory = '../master-data/aFRR_activation/', direction = \"Up\") #  a dataframe of the activation volumes for afrr up for each hour in the timeframe\n",
    "afrr_activation_down = utils.get_afrr_activation_data(tf = timeframe, afrr_directory = '../master-data/aFRR_activation/', direction = \"Down\") #  a dataframe of the activation volumes for afrr down for each hour in the timeframe\n",
    "frequency_quarter_dict = utils.find_frequency_quarters(freq_df = freq_data, hours = H, index = True) # a dictionary of the frequency quarters for each hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afrr_activation_up.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(x[7, l, 21].X for l in range(len(L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(L) * y[7, 21].X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round(sum(x[7, 839, m].X for m in range(len(M))), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract binary variable values from the original model\n",
    "#new_x_values = {(h, l, m): test_model.getVarByName(f\"x_{h}_{l}_{m}\").X for h in range(len(H)) for l in range(len(L)) for m in range(len(M))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Load the saved values\n",
    "with open('current_x_values_for_week.pkl', 'rb') as f:\n",
    "    original_x_values = pickle.load(f)\n",
    "\n",
    "old_dict = utils.get_market_count_dict(original_x_values)\n",
    "new_dict = utils.get_market_count_dict(new_x_values)\n",
    "\n",
    "differences = {}\n",
    "for key in old_dict:\n",
    "    if not old_dict[key].equals(new_dict[key]):\n",
    "        differences[key] = (new_dict[key], old_dict[key])\n",
    "        \n",
    "\n",
    "for key, (orig_val, mod_val) in differences.items():\n",
    "    print(f\"Difference for hour {key}: \\n Original={display(orig_val)}, \\n  Modified={display(mod_val)}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Extract binary variable values from the original model\n",
    "current_x_values = {(h, l, m): test_model.getVarByName(f\"x_{h}_{l}_{m}\").X for h in range(len(H)) for l in range(len(L)) for m in range(len(M))}\n",
    "\n",
    "# Save these values\n",
    "with open('current_x_values_for_week.pkl', 'wb') as f:\n",
    "    pickle.dump(current_x_values, f)\"\"\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF264",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
