{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this version I will integrate a neural netwrok to my reinforcement learning model to enhance its ability to capture complex patterns and relationships in the data thar a linear model might miss. This approach is commonly known as a Deep-Q-Netork (DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "import pandas as pd\n",
    "from code_map import final_markets, new_meters, utils, weather, timeframes, rl_utils\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = timeframes.one_week\n",
    "L, M, F, H, freq_data, power_meter_dict, consumption_data = utils.get_all_sets(timeframe= tf, areas = [\"NO5\"])\n",
    "L_u, L_d, Fu_h_l, Fd_h_l, R_h_l, P_h_m, Vp_h_m, Vm_m, R_m = utils.get_parameters(L = L, M = M, H = H)\n",
    "Ir_hlm, Ia_hlm, Va_hm = utils.get_income_dictionaries(H=H, L = L, M = M, freq_data= freq_data, Fu_h_l= Fu_h_l, Fd_h_l= Fd_h_l, P_h_m= P_h_m, Vp_h_m= Vp_h_m, F = F, markets_dict = {market.name : market for market in M}, timeframe = tf, areas = [\"NO5\"])\n",
    "compatible_dict = utils.get_compatibility_dict(L = L ,M = M, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_market_names = [\"FCR\", \"aFRR\"]\n",
    "markets = [market for market in M if sup_market_names[0] in market.name  or sup_market_names[1] in market.name]\n",
    "exp_price_dict, exp_vol_dict = rl_utils.get_expected_prices_and_volumes_dict(bid_timeframe= H, markets= markets)\n",
    "norm_exp_price_dict = rl_utils.normalize_dict_vals(exp_price_dict, norm_method= \"min_max\")\n",
    "norm_exp_vol_dict = rl_utils.normalize_dict_vals(exp_vol_dict, norm_method= \"min_max\")\n",
    "weather_data = weather.get_weather_data(tf= tf, areas = [\"NO5\"])\n",
    "norm_w_df = rl_utils.normalize_weather_data(weather_data= weather_data)\n",
    "spot_path = \"../master-data/spot_data/spot_june_23.csv\"\n",
    "norm_da_df = final_markets.preprocess_spot_data(pd.read_csv(spot_path), year = tf.year, start_month = tf.start_month, end_month = tf.end_month, start_day = tf.start_day, end_day = tf.end_day, start_hour = tf.start_hour, end_hour = tf.end_hour, area = \"NO5\", normalize= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = [market for market in M if sup_market_names[0] in market.name  or sup_market_names[1] in market.name]\n",
    "print([m.name for m in markets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 3\n",
    "n_features = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.output = nn.Linear(hidden_sizes[1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network\n",
    "input_size = n_features\n",
    "output_size = n_actions\n",
    "hidden_sizes = [128, 64] # Example sizes; you can adjust these\n",
    "q_network = QNetwork(input_size, output_size, hidden_sizes)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "\n",
    "# During training, collect experiences and store them in a replay buffer\n",
    "replay_buffer = []\n",
    "\n",
    "# When it's time to update the model...\n",
    "batch = random.sample(replay_buffer, batch_size)  # batch_size could be, for example, 32\n",
    "states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "# Convert to tensors\n",
    "states = torch.FloatTensor(states)\n",
    "actions = torch.LongTensor(actions)\n",
    "rewards = torch.FloatTensor(rewards)\n",
    "next_states = torch.FloatTensor(next_states)\n",
    "dones = torch.FloatTensor(dones)\n",
    "\n",
    "# Compute Q values for current states\n",
    "current_q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "# Compute Q values for next states using target network\n",
    "next_q_values = target_network(next_states).max(1)[0].detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = QNetwork(input_size, output_size, hidden_sizes)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "# Compute Q values for next states using target network\n",
    "next_q_values = target_network(next_states).max(1)[0].detach()\n",
    "targets = rewards + gamma * next_q_values * (1 - dones)  # (1 - dones) to zero out terminal states\n",
    "# Periodically update target network with weights from the main network\n",
    "if episode_n % target_update_frequency == 0:\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
